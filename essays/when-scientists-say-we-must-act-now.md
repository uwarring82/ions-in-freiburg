---
description: What Are They Actually Claiming?
---

# When Scientists Say "We Must Act Now"

{% hint style="info" %}
**Stewardship:** U. Warring.\
**Version:** 1.2 — February 2026\
\
&#xNAN;_&#x41; case study in reading science news carefully, using consciousness research as an example._
{% endhint %}

***

### The headline

In early 2025, a team of prominent researchers published a paper in _Frontiers in Science_ calling for faster progress in consciousness research. The argument, widely presented in media coverage as a call for urgent action, went roughly like this:

> Artificial intelligence is advancing rapidly. Brain organoids are being grown in labs. Patients in comas may be conscious but unable to tell us. If we don't develop reliable ways to detect consciousness soon, we risk creating suffering we can't see, ignoring patients who are aware, and building machines that might experience something we never intended.

The lead author, Prof. Axel Cleeremans, warned of potential "existential risk" from accidentally creating conscious systems. His co-authors, Prof. Anil Seth and Prof. Liad Mudrik, called for adversarial collaborations — structured disagreements between rival theories — to accelerate progress.

The authors themselves are explicit about current limitations and the need for rigorous testing. The escalated language — "existential risk," "we must act now" — tends to sharpen when these ideas are translated into headlines and institutional communications. That translation process is itself worth paying attention to.

It all sounds important. But what is the scientific claim, exactly? And how close are we, really, to being able to detect consciousness in anything?

***

### The problem with urgency

Urgency is not a scientific finding. It is a feeling — sometimes justified, sometimes not. When a paper says "we must act now," the obvious follow-up is: _act on what, exactly, and on what evidence?_ Without that, urgency is just anxiety with footnotes.

This is not a criticism of the researchers involved. They are serious scientists working on genuinely difficult problems, and their paper is careful about acknowledging what remains unknown. But there is a difference between two things that often get mixed together in public discussion:

1. **The scientific question:** Can current theories of consciousness produce reliable, agreed-upon tests for detecting it?
2. **The ethical concern:** What should we do if those tests don't exist yet but the stakes are high?

These are both real questions. But they require different kinds of answers. The first is an empirical question — it can be tested. The second is a question of values and risk tolerance — it requires judgement. Mixing them together makes both harder to think about clearly.

So let us separate them — keeping in mind that separating them does not mean the ethical question disappears. We will come back to it.

***

### What is the actual testable claim?

When you set aside the urgency and the ethical framing, the scientific core of the argument comes down to something like this:

> **Current theories of consciousness can produce convergent criteria — tests that different theories would broadly agree on — for detecting consciousness in systems that cannot tell us whether they are conscious.**

That is a specific, testable claim. Either the leading theories produce overlapping detection criteria, or they don't. Either those criteria give consistent results when different labs apply them, or they don't.

This is much narrower than what the headline suggests. The headline says "scientists warn of existential risk." The testable claim underneath is about whether two theories agree on what counts as evidence of consciousness. Those are very different statements, and the gap between them matters.

***

### Two theories, two very different ideas

The paper draws on two leading theories of consciousness. Understanding what they actually say helps clarify whether convergence is likely.

**Global Workspace Theory** (GWT), originally proposed by Bernard Baars, says consciousness is what happens when information gets broadcast widely across the brain. Think of it like a stage in a theatre: most of what the brain does happens backstage, out of awareness. Consciousness is what makes it onto the stage — the information that gets shared with all the different brain systems at once. On this view, detecting consciousness means detecting that kind of widespread information sharing.

**Integrated Information Theory** (IIT), developed by Giulio Tononi, takes a fundamentally different approach. It says consciousness is not about what information _does_ (getting shared) but about how a system _is_ (how tightly its parts are woven together). On this view, any system with enough internal integration — enough of a property IIT calls Φ (phi) — is conscious to some degree, in principle regardless of whether it implements the kind of global broadcasting that GWT emphasises.

These are not two versions of the same idea. They disagree about what consciousness fundamentally _is_: GWT treats it as a function — something the brain does; IIT treats it as a structure — something a system has. That is a deep difference, and it matters for what happens when you try to build a test.

That said, the two properties can co-occur. A system that broadcasts information widely might also be highly integrated, and in practice — especially in healthy human brains — both signatures are likely to be present. The question is not whether the properties can overlap in some cases, but whether the theories _systematically_ agree on which systems count as conscious across the full range of hard cases: patients in comas, brain organoids, AI systems, animals with very different neural architectures.

***

### So do the tests agree?

If GWT is right, you would look for signs of widespread neural broadcasting: certain patterns of brain activity that indicate information is being shared across distant regions. If IIT is right, you would measure the degree of integration in a system — a very different kind of measurement that, in principle, does not even require a brain.

Now, it is entirely possible that these two different measurements pick out overlapping systems in many practical cases. Perhaps in familiar contexts — adult humans, standard clinical settings — the two approaches would agree well enough to guide decisions. That kind of _partial convergence_ in restricted domains would already be practically useful, even if the theories disagree about harder cases.

But the strong claim — convergent detection criteria that work across the full range of non-communicating systems — would need to be demonstrated, not assumed. The theories themselves do not predict it.

To test this properly, you would need a study where both kinds of measurements are applied to the same subjects — ideally subjects who cannot simply tell you whether they are conscious, since that is the hard case the whole debate is about. You would need multiple labs running the same protocols, and you would need to agree in advance on what counts as "the theories converge" versus "the theories diverge."

Has that been done?

***

### The landmark attempt to put the theories head-to-head

A major effort was made. Between 2019 and 2023, the Templeton Foundation funded a large-scale adversarial collaboration — a structured programme designed to put the theories into direct empirical contact using pre-registered experiments and agreed-upon criteria.

The results, reported by the Cogitate Consortium, were instructive. The experiments challenged strong predictions from both theories, narrowed the viable parameter space for each, and produced meaningful constraints on what the theories can claim. That is genuine scientific progress — the kind that happens when competing frameworks are forced to make precise, testable predictions rather than vague ones.

What the programme did not produce was a clean convergence — a set of results that both theories would agree constitutes evidence of consciousness in the same systems. Nor did it produce a clean divergence — a clear case where the theories point in unambiguous opposite directions. The picture was partial and complicated, as first-generation adversarial results in a young field tend to be.

In other words: the single largest, best-funded attempt to test whether these theories converge on detection criteria made progress, but did not resolve the question.

***

### What does that tell us?

It tells us that the testable claim — "current theories yield convergent detection criteria" — is currently **underdetermined**. That is a precise term, and it means something specific:

* It does _not_ mean the claim is wrong. The theories might converge, especially in restricted domains where both signatures co-occur. We do not yet have the decisive evidence to say.
* It does _not_ mean the claim is right. The theories might diverge fundamentally on the hard cases. We do not yet have the decisive evidence to say that either.
* It _does_ mean that the evidence base is currently compatible with multiple, competing ways of telling the story. Underdetermined says nothing about which story will survive as more evidence arrives.

And here is the important part: the study which would move this forward — multiple labs, both measurement types, same subjects, pre-registered agreement thresholds — is feasible. It could be done now, with existing technology and methods. It is not waiting for some future breakthrough. It is waiting for coordination, funding, and the willingness of rival research groups to commit to shared criteria for what would count as success or failure.

***

### But what about the ethics?

Here is where we need to come back to the question we set aside.

The analysis above addresses the _scientific_ claim: do our theories agree on how to detect consciousness? The answer, for now, is: not yet, but a decisive test is feasible.

But the _ethical_ concern — should we be cautious about creating potentially conscious systems? — does not necessarily depend on that scientific question being resolved first.

Many precautionary arguments in ethics and policy do not require scientific certainty. They depend on something weaker but still powerful: _plausible risk under uncertainty_. We apply this logic in biosafety, environmental regulation, and animal welfare. We do not wait for proof that a new chemical causes cancer before restricting it; we act when the risk is plausible and the potential harm is serious enough to warrant caution.

The same logic could apply to consciousness. Even if we cannot yet reliably detect it, the _possibility_ that certain systems might be conscious — combined with the severity of the harm if we are wrong — could be enough to justify precautionary measures. That argument does not require the science to be mature. It requires only that the risk be taken seriously.

So the relationship between the scientific finding (underdetermined) and the ethical conclusion (act with caution) is more nuanced than it first appears. Scientific uncertainty does not automatically justify inaction. It depends on what is at stake, how severe the potential harm is, and what the costs of caution are. Those are questions of values, not of measurement — and reasonable people can disagree about them.

What the scientific analysis _does_ tell us is that anyone claiming "the science clearly shows we must act" is overstating what the science currently delivers. Precaution can be justified without that overstatement. In fact, the honest version of the argument — "we don't yet know, and that uncertainty itself is a reason for care" — is more robust than the inflated one.

***

### A different kind of question

There is another way to approach this territory — one that sidesteps the consciousness debate entirely and asks a more tractable question first.

Instead of asking "is this system conscious?" — which requires solving a problem that philosophy and neuroscience have not yet solved — you can ask: "does this system have the structural conditions to maintain itself through its own internal dynamics?" That is a physics question. It is measurable. And it turns out to be surprisingly discriminating.

This is the approach taken by the [Ordinans framework](https://uwarring.gitbook.io/ions-in-freiburg/invariant-frameworks/g-what-can-make-a-complex-system-an-ordinans), an emerging research programme under development at the University of Freiburg's Institute of Physics. The framework identifies five necessary conditions that a system must satisfy before it can even be a candidate for what it calls _sui-ordinatio_ — the capacity to sustain its own large-scale organisation through internal dynamics rather than external support:

| Condition                                | Example in a brain                      | Why a large language model fails              |
| ---------------------------------------- | --------------------------------------- | --------------------------------------------- |
| Hybrid structure (continuous + discrete) | Membrane potentials + action potentials | Uniformly continuous internal processing      |
| Defined spatial extent                   | Anatomical boundaries                   | Arguably satisfied, but operationally unclear |
| Intrinsic timescale                      | Neural oscillations, refractory periods | No internal clock; inert between prompts      |
| Non-trivial persistent topology          | Synaptic connectivity evolves slowly    | Network rebuilt from scratch each inference   |
| Thermodynamic openness                   | Continuous metabolic exchange           | Effectively closed during operation           |

These are not philosophical objections. They are structural observations about what kind of system it is. A living cell satisfies all five conditions. Your brain satisfies all five. A large language model fails multiple conditions — and the specific failures tell you something important about the gap between capability and self-maintenance.

The framework goes further. Systems satisfying all five conditions can occupy three distinct correlation regimes — local, scale-free, and global — and the signature of a self-maintaining system (an _ordinans_) is that it cycles between these regimes in a characteristic pattern, retaining structural traces from each cycle. This is measurable through hysteresis: the path into a high-coherence state differs from the path out, and the difference represents something preserved.

What makes this approach interesting in the context of the consciousness debate is what it deliberately does _not_ do. The framework makes no claim that satisfying its conditions makes a system conscious. It explicitly states that a system could exhibit perfect structural self-maintenance and possess no inner experience whatsoever. It separates the structural question — which admits empirical answers — from the phenomenological question — which currently does not.

That separation is exactly what the consciousness detection debate is missing. The Frontiers paper calls for "evidence-based tests to identify consciousness." The Ordinans framework suggests that before we can build such tests, we need to answer a prior question: what kind of system is even a candidate? If a system fails the basic structural conditions for self-maintenance, the question of whether it is conscious may not even be well-posed.

This does not resolve the ethical dilemma. A system might lack sui-ordinatio and still deserve moral consideration for other reasons. But it does something valuable: it provides a concrete, falsifiable intermediate step in a debate that has been stuck between rival theories with no agreed-upon way to test them. Instead of waiting for GWT and IIT to converge — which may take decades — we can start by mapping which systems have the structural architecture that makes the question of consciousness relevant in the first place.

***

### The gap between the claim and the headline

Now step back and look at the distance between what we found and what the news coverage suggested.

The headline: **"Scientists warn of existential risk from conscious AI."**

What the testable claim actually says: **"We do not yet know whether our best theories of consciousness agree on how to detect it."**

What the evidence shows: **"The major attempt to find out made progress but did not resolve the question. A decisive study is feasible."**

That is a noticeably different story. It is not a story about existential risk. It is a story about a scientific programme that is still in progress — one where important work has been done, but the key question remains open.

The ethical concerns may well be valid — but they stand on their own feet, as precautionary arguments under uncertainty, not on the claim that consciousness science is ready to guide action. Recognising that distinction does not diminish the ethics. It strengthens them, by freeing them from a scientific foundation that cannot yet bear the weight.

***

### How to read the next headline

This gap between confident headlines and open scientific questions is a feature of many frontier disciplines, not just consciousness research. The pattern recurs: a genuine question gets wrapped in urgency and presented to the public as a crisis that demands immediate action. The urgency may be warranted; the scientific readiness to address it often is not.

Here are a few questions worth asking whenever you encounter a headline that says "scientists warn" or "researchers urge":

**What is the testable claim?** Every call to action rests on some factual belief about the world. What is it? Can you state it in one sentence without using words like "urgent," "crucial," or "existential"?

**What would it take to check?** Is there a specific observation or experiment that would tell you whether the claim is true? If so, has it been done? If not, is it feasible?

**What do the rival theories predict?** In any active scientific field, there are competing explanations. Do they agree on the relevant point, or do they disagree? If they disagree, that does not mean the claim is wrong — but it means the field has not converged, and confident assertions based on unconverged science carry their own risks.

**Does the ethical argument depend on the science being settled?** Sometimes it does, sometimes it doesn't. Precautionary arguments can be valid even when the science is uncertain — but they should be honest about that uncertainty rather than borrowing confidence from science that hasn't yet delivered it.

***

### A worked example: what different outcomes would mean

To make the consciousness case concrete, here is what three different outcomes of a decisive study would mean.

**If the theories' detection criteria converge** — if GWT-based and IIT-based markers identify the same systems as conscious across a large, pre-registered study — then the claim is compatible with the evidence. That does not prove either theory is correct about the _nature_ of consciousness, but it would mean we have a practical detection tool that works regardless of which theory turns out to be right. That would be a genuine breakthrough with real implications for patients, animals, and AI.

**If the theories' detection criteria diverge cleanly** — if GWT says "this system is conscious" and IIT says "this system is not," consistently and reproducibly — then the claim is inconsistent with the evidence. The theories do not converge. Any detection protocol based on one theory would disagree with a protocol based on the other. At that point, choosing which test to use would be a philosophical decision, not a scientific one, and the ethical arguments would need to be rebuilt on different foundations.

**If the results show partial convergence** — agreement in some domains (say, adult humans under clinical conditions) but disagreement in others (say, brain organoids or AI systems) — then the picture is more nuanced. For the restricted domains where agreement holds, practical detection tools could already be developed. For the broader claim, the question remains open. This is perhaps the most likely outcome, and it would mean that the answer depends on _which systems you are asking about_ — a possibility that the headline's sweeping framing obscures entirely.

Right now, we are closest to that third space. The question is partially open, partially constrained. The experiment that would clarify it further is feasible. The answer is not yet in.

***

### What this exercise demonstrates

We started with a news headline about existential risk and ended with a clear, specific, testable question and an honest assessment of where the evidence stands. Nothing in between required expertise in neuroscience, philosophy, or statistics. It required only the willingness to slow down and ask: _what is actually being claimed, and what would it take to check?_

That method — extracting the testable core from a rhetorical wrapper, identifying what would count as evidence, and checking whether that evidence exists — is not limited to consciousness research. It works on any scientific claim, in any field, at any level of complexity. The details change; the structure does not.

One important caveat: this essay has adopted a particular perspective — that scientific claims should be evaluated against evidence before they are used to justify action. That is a defensible position, but it is not the only one. Others would argue that in the face of serious potential harms, the burden of proof runs the other way: we should act cautiously until the science tells us it is safe not to. Both positions are legitimate. What neither position can honestly do is claim that the current state of consciousness science has settled the matter. It has not. The work continues.

***

_This analysis follows the Claim Analysis Ledger protocol developed by the Open-Science Harbour. It is a prose translation of a more formal entry, CL-2026-001, available in the Breakwater section of the Harbour documentation. The Ledger is a measurement instrument: it classifies claims as compatible, underdetermined, or inconsistent with established constraints. It does not rank authors, recommend actions, or endorse positions. This essay sits in the Sails layer — interpretation and context — and reflects the author's reading of the Ledger result, not the Ledger itself._
