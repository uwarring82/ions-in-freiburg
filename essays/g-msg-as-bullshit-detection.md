---
description: Provenance Notation as Epistemic Hygiene
---

# G–MSG as Bullshit Detection

{% hint style="info" %}
**Author:** U. Warring\
**Affiliation:** Institute of Physics, University of Freiburg\
**Version:** 0.5.1\
**Last updated:** 2026-01-03\
**License:** CC BY 4.0\
**Status:** Essay (Sail)\
**Parent Framework:** G–LLM Integration in Foundational Education\
**Non-status:** ❌ Not a framework · ❌ Not policy · ❌ Not curriculum
{% endhint %}

***

### 1. The Epistemic Envelope

Gödel's incompleteness theorems (1931) established that no sufficiently powerful formal system can prove its own consistency from within. A system that could verify all its own truths would either be incomplete or inconsistent. This is a structural boundary, not a temporary limitation.

***

**Boundary Statement**

Hilbert's gravestone reads:

> _Wir müssen wissen. Wir werden wissen._\
> ("We must know. We will know.")

The inversion:

> _We must know. We cannot fully know._

This is an **envelope**, not a warrant. The envelope marks where self-verification ends; it does not tell us what lies beyond. What lies beyond is the domain of external warrant — evidence, argument, and correction from sources not generated by the system's own outputs.

***

For human reasoning, the external warrant gate is the encounter with evidence that does not originate in one's own assumptions. For AI-assisted reasoning, the gate is the human who provides warrant independent of the model's output.

MSG (Minimal Schematic Grammar) operationalises this boundary. The S? marker flags content whose warrant status is internal to the model — pattern-matched, plausible, but not externally verified. The S→ conversion is the gate: human warrant from outside the model's inference chain. MSG makes this gate legible: S? marks what is still inside the model; S→ marks what has passed through the human.

This framing is not mysticism about AI limits. It is recognition that self-referential verification has structural boundaries, and that disciplined reasoning requires marking where those boundaries lie.

***

### 2. Epistemic Pollution and the Consolidation Cycle

The production of plausible but unwarranted claims is not new. History shows a recurring cycle driven by social amplification:

**Fragmentation:** Information sources multiply. Printing, pamphlets, newspapers, radio, television, the web — each expansion increases the volume of claims competing for attention. Quality control fragments; provenance becomes harder to trace. Pamphlet wars, newspaper circulation races, and algorithmic feed optimisation share a social driver: attention incentives reward volume over verification long before any particular architecture intensifies the slope.

**Consolidation:** Institutions emerge to filter, verify, and organise. Universities, scientific societies, editorial boards, peer review. These reduce noise by imposing warrant requirements. Consolidation is also social: trust networks, credentialing, shared standards.

**Renewed fragmentation:** New technologies lower production costs, bypass gatekeepers, and restart the cycle.

LLMs are the current accelerant. They amplify the fragmentation slope by making plausible content nearly free to produce. But they are not the origin of epistemic pollution; they are a new chapter in an old pattern.

This framing determines response. If LLMs were uniquely dangerous, restriction might suffice. If they amplify a recurring social dynamic, we need scalable consolidation tools — methods that reimpose warrant requirements without bottlenecking on slow institutional review.

Under disciplined consolidation, LLMs accelerate synthesis; under undisciplined Extract, they accelerate pollution. The difference is not the tool but the gate.

MSG is a consolidation tool. It marks where warrant is missing and blocks elevation until the gate is passed.

***

### 3. MSG as Reasoning Interface

The Minimal Schematic Grammar provides six primitives:

<table><thead><tr><th width="99.33984375">Symbol</th><th width="302.1328125">Meaning</th><th>Epistemic Status</th></tr></thead><tbody><tr><td>B</td><td>Base — raw data or source material</td><td>Delegable (traceability required)</td></tr><tr><td>S</td><td>Signal — human-originated relevance selection</td><td>Non-delegable (human origin)</td></tr><tr><td>S?</td><td>Pending Signal — model-suggested, awaiting ratification</td><td>Blocked (awaiting warrant)</td></tr><tr><td>S→</td><td>Ratified Signal — model-suggested, human-verified</td><td>Non-delegable (requires independent warrant)</td></tr><tr><td>R</td><td>Representation — processed output</td><td>Delegable</td></tr><tr><td>K</td><td>Knowledge — elevated claim</td><td>Non-delegable (requires complete B→S→R chain)</td></tr></tbody></table>

The directional lock: no pathway from B to K bypasses a human-verified Signal gate. The model may suggest; the human must ratify.

***

### 4. The Efficiency Asymmetry

The core argument for MSG is economic, not moralistic.

**Tagging is cheap.** Marking a model suggestion as S? takes seconds. Noting the warrant for S→ conversion takes a sentence.

**Remediation is expensive.** Fact-checking a published claim, correcting a flawed analysis, retracting a paper, rebuilding trust — these cost hours, months, or years.

MSG shifts cost upstream, where it is smallest, and prevents propagation downstream, where it compounds. The asymmetry favours early detection over late correction.

***

### 5. Failure Modes

**False ratification:** Converting S? to S→ without genuine independent warrant. Signs: "It sounded right," "Another model agreed," "I checked the citation exists" (without checking what it says).

**Extract-smuggling:** Model performs relevance selection inside nominally delegable Process. Signs: ranked lists accepted without human-specified criteria, "key findings" adopted without asking who defined "key."

**Metric gaming:** S? closure rate becomes a target; producers rubber-stamp to hit numbers. Protection: read metrics as a system; audit warrant quality, not warrant presence.

**Failure theorem:** Under bad faith, any notation system — including MSG — can be used to generate well-documented nonsense. The notation makes the absence of warrant visible; it cannot compel its presence.

***

### 6. Cultural Showcases

Literature provides intuitive wedges for Extract awareness. These are readability aids, not proofs — they make abstract threats tangible without requiring technical fluency.

**Swift, "A Modest Proposal" (1729):** Reasonable tone masks absurd framing. The reader who accepts the "findings" without questioning who decided children are economic units has been smuggled into an untenable position. _Probe: Who set the relevance criteria?_

**Joyce, Ulysses, "Ithaca" (1922):** Catechetical format lends false authority to trivial content. Format-as-authority is the genre trick LLMs inherit — confident prose regardless of epistemic status. _Probe: Does format match warrant?_

**Borges, "Tlön, Uqbar, Orbis Tertius" (1940):** A fictional encyclopaedia, internally coherent, gradually displaces reality. Coherence is not warrant; systematicity is not truth. _Probe: Is internal consistency being mistaken for external verification?_

**Orwell, "Politics and the English Language" (1946):** Clichés and prefabricated phrases bypass thought. The writer who reaches for stock expressions has outsourced their reasoning to linguistic habit. LLM outputs are industrialised cliché — statistically average phrasings that feel natural because they are familiar. _Probe: Is this phrasing chosen or defaulted?_

Assigning these texts before introducing MSG provides non-technical grounding for provenance awareness.

***

### 7. Independent Warrant: Scalar, Not Binary

Independence is not absolute. Corroboration sources vary in how much they diverge from the original model output. The S→ annotation carries a warrant-quality label:

```
S→(warrant: high-independence)
S→(warrant: low-independence)
```

**High-independence criteria** (measurable divergence):

* Retrieval corpus overlap less than 50% (by source hash or document ID)
* Training data cut-offs at least 6 months apart
* Different representational principles (symbolic vs neural, or materially disjoint corpora)
* Human expert with domain knowledge not derived from the same model family

**Low-independence indicators:**

* Same model family or shared training corpus
* Corroboration from a second LLM where corpus overlap was not measured
* Citation exists but source content not verified

**Non-examples (not warrant at any level):**

* "It sounds right"
* "The model is confident"
* "Everyone knows this"

**Practical supports:**

* Log your sources (URL, page number, conversation).
* Timestamp your ratifications.
* Write one sentence of reasoning for each S→, including warrant level.

The scalar label makes warrant quality visible without imposing a binary gate that rejects useful low-independence corroboration. Low-independence S→ items may proceed but are flagged for higher scrutiny at consolidation checkpoints.

***

### 8. Team-Scale Governance

Individual MSG discipline scales to teams with three constructs:

**8.1 Team S? Density (TSD)**

The ratio of S? items to total claims in a draft. High TSD signals exploratory mode — not ready for elevation. Low TSD after LLM-heavy drafting signals potential under-tagging.

**8.2 S? Consolidation Checkpoint**

A scheduled review (e.g., before submission) where all remaining S? items are either converted with documented warrant, explicitly discarded, or flagged as open questions. No unclosed S? in elevated claims.

**8.3 S? Clustering Protocol**

In high-density synthesis (literature reviews, exploratory analysis), S? items can be grouped into clusters for batch review rather than atomistic resolution:

```
S?{cluster: "candidate-causal-links"} → n model-suggested connections
S?{cluster: "emerging-themes"} → k ranked themes (criteria pending)
```

Clusters are batch-review regions resolved at consolidation checkpoints. The cluster label makes explicit that these items share a common warrant gap — they will be evaluated together, not individually. This reduces cognitive overhead without hiding the pending status.

**8.4 Warrant Audit Participation Rate**

The fraction of S→ conversions reviewed by someone other than the original author, sampled periodically. A 10–20% sample rate catches systematic false-ratification without creating bottlenecks.

These constructs are diagnostic, not punitive. They surface where the team's epistemic hygiene is strong or weak.

***

### 9. Smarter Models

As models improve — calibration, retrieval-augmentation, uncertainty quantification — does MSG become unnecessary?

No. Model confidence is information about the model, not about the world. A model reporting "high confidence" means the pattern was strong in training data. It does not mean the claim is warranted.

The asymmetry holds:

* Low model confidence can prioritise verification (check this first).
* High model confidence cannot bypass verification (still S?).

Smarter models help with triage. They do not transfer warrant. The external gate remains human.

***

### 10. Integration After Fragmentation

A note of balance: LLMs can accelerate consolidation, not only fragmentation.

When governed by human-specified relevance criteria, LLMs can reconcile information across silos, surface connections humans would miss, and draft syntheses that would take weeks to produce manually. The problem is not the tool but the absence of the Extract gate. Under disciplined use, LLMs are powerful consolidation aids — processing volume while humans retain judgement.

MSG does not oppose LLM use. It structures it. The goal is not to slow production but to ensure that what is produced has traceable warrant.

***

### 11. Conclusion

Epistemic pollution is cyclical. Technologies fragment; institutions consolidate; new technologies fragment again. The driver is social — attention, incentive, trust — before it is technological.

LLMs are the current accelerant. MSG is a consolidation tool — a notation that marks where warrant is missing and blocks elevation until the gate is passed. It is cheap to apply, expensive to skip, and robust to model improvement.

But no notation compels honesty. Under bad faith, any system produces well-documented nonsense. MSG makes this legible early. It surfaces where warrant is missing before claims propagate.

The cycle will continue. New tools will lower production costs; new consolidation methods will be needed. The discipline is not to abdicate inquiry to the latest system, nor to reject systems wholesale, but to mark where warrant is required and to provide it.

Each cycle begins with the same boundary: we must know, and we cannot fully know. The rest is the work of warrant.

Epistemic hygiene is not a finishable task but a practice renewed each cycle.

***

### References

* Gödel, K. (1931). "Über formal unentscheidbare Sätze der Principia Mathematica und verwandter Systeme I."
* Frankfurt, H. (1986/2005). _On Bullshit_. Princeton University Press.
* Orwell, G. (1946). "Politics and the English Language."
* Gebru, T. et al. (2018). "Datasheets for Datasets." arXiv:1803.09010.
* Floridi, L. & Chiriatti, M. (2020). "GPT-3: Its Nature, Scope, Limits, and Consequences."
* Bender, E. et al. (2021). "On the Dangers of Stochastic Parrots."

***

### Status

This essay is a Sail — interpretive, revisable, deriving authority from use. It serves as one key to the G–LLM Integration framework lock. Other interpretations of MSG remain valid.

***

### Revision History

<table><thead><tr><th width="104.00390625">Version</th><th width="119.66796875">Date</th><th>Summary</th></tr></thead><tbody><tr><td>0.1.0</td><td>2026-01-03</td><td>Initial draft</td></tr><tr><td>0.2.0</td><td>2026-01-03</td><td>Technical additions: provenance objects, institutional pipeline</td></tr><tr><td>0.3.0</td><td>2026-01-03</td><td>Accessibility revision: trust framing, model confidence section</td></tr><tr><td>0.4.0</td><td>2026-01-03</td><td>Structural revision: Gödelian envelope, consolidation cycle, team-scale governance</td></tr><tr><td>0.5.0</td><td>2026-01-03</td><td>Refinements: scalar warrant labels, S? clustering protocol, social-amplification framing</td></tr><tr><td>0.5.1</td><td>2026-01-03</td><td>bridging sentence §1, fused §2, table wording §3, failure theorem §5, independence phrasing §7, envelope tie-back §11; marked stable</td></tr></tbody></table>

***

_End of essay._
